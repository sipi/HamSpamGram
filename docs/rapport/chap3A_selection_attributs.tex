\label{partieA3}
\textit{L'objectif de la sélection d'attributs est de déterminer un sous-ensemble d'attributs pertinents permettant ainsi de réduire le nombre de dimensions des données.}

\subsection{Étude préalable -- Tests}

Dans le but de travailler plus rapidement lors des tests, nous avons effectué cette analyse préalable sur la version v.0.1 du corpus, ne comportant que 322 Spam et 1002 Ham.

Lors de cette étude, nous avons testé quatre méthodes de recherche avec leur méthode d'évaluation par défaut.

\subsubsection{BestFirst -- CfsSubsetEval}

La méthode \texttt{BestFirst} examine l'espace de recherche à l'aide de l'heuristique de \og greedy hillclimbing \fg{} avec la possibilité de faire un  \og backtrack \fg{}.  Pour ce premier test, nous avons choisi de commencer avec un ensemble d'attributs vide et de procéder par recherche en avant.

Paramètres de l'algorithme : 
\begin{description}
	\item[direction] = Forward
	\item[startSet] = (vide)
\end{description}

La méthode d'évaluation \texttt{CfsSubsetEval} évalue les sous-ensembles d'attributs générés par la mesure de dépendance entre les attributs.

\subsubsection{GreedyStepwise -- CfsSubsetEval}

La méthode \texttt{GreedyStepwise} examine l'espace de recherche par l'heuristique \og greedy \fg{}. Pour ce test, nous avons choisi de commencer avec un ensemble d'attributs vide, de procéder par recherche en avant et de ne pas générer de \og ranking \fg{} sur nos attributs.

Paramètres de l'algorithme :
\begin{description}
	\item[generateRanking] = False
	\item[searchBackwards] = False
	\item[startSet] = (vide)
\end{description}

La méthode d'évaluation utilisée est la même que pour \texttt{BestFirst} : \texttt{CfsSubsetEval}.

\subsubsection{LinearForwardSelection -- CfsSubsetEval}

La méthode \texttt{LinearForwardSelection} est une extension de la méthode \texttt{BestFirst}. Elle utilise un nombre $k$ d'attributs où $k$ peut être fixé selon différents critères. Pour ce test, nous avons choisi de commencer avec un ensemble d'attributs vide, de procéder par recherche en avant et de générer un \og ranking \fg{} sur nos attributs afin de sélectionner les $k=50$ premiers attributs.

Paramètres de l'algorithme :
\begin{description}
	\item[forwardSelectionMethod] = Forward selection
	\item[numUsedAttributes] = 50
	\item[performRanking] = True
	\item[startSet] = (vide)	
\end{description}

La méthode d'évaluation utilisée est la même que pour \texttt{BestFirst} : \texttt{CfsSubsetEval}.

\subsubsection{Ranker -- InfoGainAttributeEval}

La méthode \texttt{Ranker} permet de classer les attributs selon leurs évaluations. Elle offre la possibilité de classer un nombre $k$ d'attributs où $k$ peut être défini manuellement ou selon un seuil dépendant de la valeur de son évaluation. Pour voir comment varie ce nombre $k$, nous avons effectué plusieurs tests en faisant varier le seuil avec les valeurs suivantes : 

\begin{description}
	\item[threshold] = \nombre{0.005}
	\item[threshold] = \nombre{0.01}
	\item[threshold] = \nombre{0.02}
	\item[threshold] = \nombre{0.03}
	\item[threshold] = \nombre{0.05}
\end{description}

La méthode d'évaluation utilisée, \texttt{InfoGainAttributeEval}, évalue les attributs par la mesure de gain d'information par rapport à sa classe. Plus précisément, cette mesure se fait par la formule, vue en cours, suivante : 
\begin{equation*}
InfoGain(Classe,Attribut) = H(Classe) - H(Classe|Attribut)
\end{equation*}

\textbf{Variation du seuil}

\begin{center}
\begin{tabular}{c c}
\textbf{threshold} & \textbf{Nb attributs}\\
\nombre{0.005} & 356\\
\nombre{0.01} & 184\\
\nombre{0.02} & 90\\
\nombre{0.03} & 57\\
\nombre{0.04} & 37\\
\nombre{0.05} & 30\\
\end{tabular}
\end{center}

\subsection{Étude préalable -- Discussion}
L'étude des résultats des tests nous a permis de distinguer deux ensembles d'attributs distincts : 

\begin{itemize}
\item celui généré par les algorithmes utilisant la méthode d'évaluation \texttt{CfsSubsetEval}
\item celui généré par les algorithmes utilisant la méthode d'évaluation \texttt{InfoGainAttributeEval}
\end{itemize}


Les résultats obtenus par les algorithmes utilisant l'évaluation par mesure de dépendance, notamment \texttt{BestFirst}, \texttt{GreedyStepwise}, \texttt{LinearForwardSelection} avec \texttt{CfsSubsetEval}, étaient tous identiques. La différence entre ces algorithmes était visible dans leur temps d'exécution.   

Notons également la rapidité des algorithmes utilisant l'évaluation par mesure de gain d'information, comme \texttt{Ranker} avec \texttt{InfoGainAttributeEval}.


\subsection{Nos choix}

Nous avons décidé d'appliquer différentes méthodes de sélection d'attributs sur notre corpus complet afin de pouvoir comparer les résultats que nous obtiendrons après l'étape de classification. Les méthodes choisies sont les suivantes : 

\begin{enumerate}
\item Un alghorithme parmi ceux utilisant la méthode d'évaluation \texttt{CfsSubsetEval}, en l'occurence \texttt{GreedyStepwise} car plutôt rapide.
\item l'algorithme \texttt{Ranker} en faisant varier le paramètre \og threshold \fg{}.
\end{enumerate}

\subsection{Résultats}

L'ensembles des résultats appliqués au corpus complet est disponible en annexe page~\pageref{annexe:results}.