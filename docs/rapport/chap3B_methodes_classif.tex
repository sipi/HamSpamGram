\textit{L’objectif de cette étape est de générer un modèle de notre attribut CLASS en fonction des valeurs des attributs sélectionnées à l’étape précédente. Les mesures \og accuracy \fg{}, \og precision \fg{} et \og recall \fg{} nous ont permis d'évaluer plusieurs algorithmes de classification en comparant leurs performances pour chaque représentation obtenue au chapitre 2.}

\subsection{Description des algorithmes}
Parmi les algorithmes vus en cours, nous avons sélectionné \texttt{J48} et \texttt{Naïve Bayes}. Parmi ceux proposés par WEKA et non vus en cours, nous avons choisi \texttt{NBTree} et \texttt{KStar}.

\subsubsection{Arbre de décision -- J48}
L'algorithme de classification par arbre de décision \texttt{J48} est basé principalement sur l'algorithme \texttt{C4.5}. Il permet, entre autres, d'élaguer l'arbre généré afin de simplifier son interprétation. Les paramètres par défaut proposés par WEKA permettant de garder cette possibilité. Nous avons fait le choix de ne pas les modifier lors de l'exécution.

\subsubsection{Naïve Bayes}
L'algorithme de classification \texttt{Naive Bayes} génère un modèle avec la valeur qui maximise la probabilité conditionnelle de l'attribut \texttt{CLASS} sachant celle des autres attributs. Il calcule ces probabilités à l'aide du théorème de Bayes en supposant que les attributs soient indépendants.

\subsubsection{NBTree}
L'algorithme de classification \texttt{NBTree} est un algorithme hybride qui combine la classification par arbre de décision avec celle de \texttt{Naïve Bayes}. Il génère un arbre de décision qui utilise la classification par \texttt{Naïve Bayes} sur les instances des feuilles de l'arbre.

\subsubsection{KStar}
L'algorithme de classification \texttt{KStar} génère une classification basée sur toutes les instances du corpus. La classe de l'instance testée est basée sur celle des instances qui font partie du \og training set \fg{}, qui sont les plus \og proches \fg{} de cette instance. En particulier, \texttt{KStar} mesure cette distance en fonction de la mesure de l'entropie d'information entre ces instances.

\subsection{Mise en œuvre}
Afin d'appliquer ces quatre algorithmes sur l'ensemble des fichiers générés dans la partie \vref{partieA3}, nous avons décidé d'écrire un script \texttt{bash} :

//todo{Thibaut} source : script.sh

\subsection{Résultats}

Après avoir exécuté les différents algorithmes, nous pouvons faire plusieurs remarques.

\subsubsection{Sélection d'attributs}
Nous pouvons observer, grâce aux différents seuils choisis pour l'algorithme de sélection d'attributs \og Ranker \fg{}, que plus le nombre d'attributs sélectionnés est grand plus le nombre de SMS mal classés est faible.

\subsubsection{Meilleur algorithme}
    le meilleur algo semble être NBTree

\subsubsection{Meilleur représentation des données}
    le meilleur modèle semble être le boolean 

    l'algo KStar a une bonne précision sur les SPAM (très peu de ham classé comme SPAM)



